"""
VLMæ€§èƒ½æµ‹è¯•è„šæœ¬ -  ç›´æ¥æµ‹è¯•æ¨¡å‹æ€§èƒ½

åŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒVLMæ¨¡å‹
2. åœ¨Qilinæµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½
3. æ”¯æŒäº¤å‰ç¼–ç å™¨å’ŒåŒå‘ç¼–ç å™¨æµ‹è¯•
4. è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
"""

import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import argparse
import os
from transformers import AutoModelForImageTextToText, AutoProcessor
from datasets import load_dataset
from PIL import Image
import json
from collections import defaultdict
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
# å¯¼å…¥åŸå§‹è¯„ä¼°å™¨çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
def calculate_metrics(sorted_results, ground_truth, k_list):
    """
    Calculate MRR@k, MAP@k, Recall@k, Precision@k for multiple k values
    """
    max_k = max(k_list)  
    metrics = {k: {"mrr": 0.0, "map_sum": 0.0, "recall": 0.0, "precision": 0.0} for k in k_list}
    num_queries = len(ground_truth)
    valid_queries = 0

    for qid, relevant_pids in ground_truth.items():
        if qid not in sorted_results:
            num_queries -= 1  
            continue
        
        valid_queries += 1
        retrieved_pids = sorted_results[qid][:max_k]
        hits = [pid in relevant_pids for pid in retrieved_pids]

        # Calculate metrics for each k value
        for k in k_list:
            hits_at_k = hits[:k]
            
            # Calculate MRR@k
            if any(hits_at_k):
                first_hit_rank = hits_at_k.index(True) + 1  
                metrics[k]["mrr"] += 1 / first_hit_rank

            # Calculate MAP@k
            avg_precision = 0.0
            num_correct = 0
            for i, is_relevant in enumerate(hits_at_k):
                if is_relevant:
                    num_correct += 1
                    precision_at_i = num_correct / (i + 1)
                    avg_precision += precision_at_i
            if num_correct > 0:  
                avg_precision /= min(len(relevant_pids), k)  
                metrics[k]["map_sum"] += avg_precision

            # Calculate Recall@k
            metrics[k]["recall"] += sum(hits_at_k) / len(relevant_pids)

            # Calculate Precision@k
            metrics[k]["precision"] += sum(hits_at_k) / k

    # Return all zeros if no valid queries
    if valid_queries == 0:
        return {f"{metric}@{k}": 0.0 
                for k in k_list 
                for metric in ["MRR", "MAP", "Recall", "Precision"]}

    # Calculate average metrics for all k values
    results = {}
    for k in k_list:
        results[f"MRR@{k}"] = metrics[k]["mrr"] / valid_queries
        results[f"MAP@{k}"] = metrics[k]["map_sum"] / valid_queries
        results[f"Recall@{k}"] = metrics[k]["recall"] / valid_queries
        results[f"Precision@{k}"] = metrics[k]["precision"] / valid_queries

    return results

os.environ['CUDA_VISIBLE_DEVICES'] = '1'

class VLMPerformanceTester:
    """VLMæ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, model_path, device='cuda'):
        self.device = device
        self.model_path = model_path.rstrip('/')
        
        print(f"Loading VLM model from: {self.model_path}")
        
        # åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
        self.model = AutoModelForImageTextToText.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœæ˜¾å­˜
            device_map='auto'
        )
        self.model.eval()
        
        self.processor = AutoProcessor.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        # åŠ è½½æ•°æ®
        self.load_test_data()
        
        print("Model and data loaded successfully!")
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("Loading Qilin test dataset...")
        
        # åŠ è½½æ–‡æ¡£è¯­æ–™åº“
        self.corpus = load_dataset("/data1/Qilin/datasets/qilin", 'notes')['train']
        
        # åŠ è½½æµ‹è¯•æŸ¥è¯¢
        self.test_data = load_dataset("/data1/Qilin/datasets/qilin", 'search_test')['train']
        
        # åŠ è½½ç›¸å…³æ€§æ ‡æ³¨
        self.qrels = self.load_qrels("/data1/Qilin/datasets/search.test.qrels.csv")
        
        print(f"Loaded {len(self.test_data)} test queries")
        print(f"Loaded {len(self.corpus)} documents")
        print(f"Loaded {len(self.qrels)} relevance annotations")
    
    def load_qrels(self, qrels_path):
        """åŠ è½½ç›¸å…³æ€§æ ‡æ³¨"""
        df = pd.read_csv(qrels_path)
        qrels = defaultdict(list)
        for _, row in df.iterrows():
            qrels[int(row["qid"])].append(int(row["pid"]))
        return qrels
    
    def create_default_image(self):
        """åˆ›å»ºé»˜è®¤å›¾åƒ"""
        return Image.new('RGB', (1024, 1024), color='white')
    
    def load_note_image(self, image_paths):
        """åŠ è½½ç¬”è®°å›¾åƒ"""
        if not image_paths:
            return self.create_default_image()
        try:
            image_path = os.path.join('/data1/Qilin/datasets', image_paths[0])
            image = Image.open(image_path)
            image = image.resize((1024, 1024))
            return image
        except Exception as e:
            print(f"Failed to load image: {e}")
            return self.create_default_image()
    
    def test_cross_encoder_performance(self, sample_num=100):
        """æµ‹è¯•äº¤å‰ç¼–ç å™¨æ€§èƒ½ - å‚è€ƒåŸå§‹VLMè¯„ä¼°å™¨"""
        print(f"\n=== Testing Cross-Encoder Performance (Sample: {sample_num}) ===")
        
        # ä½¿ç”¨åŸå§‹è¯„ä¼°å™¨çš„æ ¼å¼
        predictions = defaultdict(list)
        
        # é€‰æ‹©æµ‹è¯•æ ·æœ¬
        test_samples = self.test_data.select(range(min(sample_num, len(self.test_data))))
        
        for idx, item in enumerate(tqdm(test_samples, desc="Testing Cross-Encoder")):
            query = item["query"]
            search_results = item.get("search_results", [])
            
            if not search_results:
                continue
            
            # è·å–å€™é€‰æ–‡æ¡£
            candidates = search_results[:100]  # é™åˆ¶å€™é€‰æ•°é‡
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            for note_idx in candidates:
                score = self.compute_cross_encoder_score(query, note_idx)
                # ä½¿ç”¨idxä½œä¸ºqidï¼Œå› ä¸ºqrelsä¸­çš„qidæ˜¯ä»0å¼€å§‹çš„è¿ç»­æ•°å­—
                qid = idx
                predictions[qid].append((note_idx, score))
        
        # æŒ‰åŸå§‹è¯„ä¼°å™¨æ ¼å¼æ•´ç†ç»“æœ
        sorted_results = {}
        for qid, preds in predictions.items():
            sorted_results[qid] = [pid for pid, _ in sorted(preds, key=lambda x: x[1], reverse=True)]
        
        # ä½¿ç”¨åŸå§‹çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
        metrics = calculate_metrics(sorted_results, self.qrels, [1, 3, 5, 10])
        
        print("\nCross-Encoder Performance:")
        print("=" * 50)
        for k, v in metrics.items():
            print(f"{k}: {v:.4f}")
        print("=" * 50)
        
        return metrics, sorted_results
    
    def test_dual_encoder_performance(self, sample_num=100):
        """æµ‹è¯•åŒå‘ç¼–ç å™¨æ€§èƒ½"""
        print(f"\n=== Testing Dual-Encoder Performance (Sample: {sample_num}) ===")
        
        # é¢„ç¼–ç æ‰€æœ‰å€™é€‰æ–‡æ¡£
        print("Pre-encoding candidate documents...")
        doc_embeddings = {}
        doc_ids = set()
        
        # æ”¶é›†æ‰€æœ‰å€™é€‰æ–‡æ¡£ID
        test_samples = self.test_data.select(range(min(sample_num, len(self.test_data))))
        for item in test_samples:
            search_results = item.get("search_results", [])
            doc_ids.update(search_results[:100])
        
        # æ‰¹é‡ç¼–ç æ–‡æ¡£
        batch_size = 8
        doc_list = list(doc_ids)
        
        # å•ä¸ªå¤„ç†æ–‡æ¡£ï¼Œä½¿ç”¨å®Œæ•´VLMåŠŸèƒ½
        for doc_idx in tqdm(doc_list, desc="Encoding documents"):
            try:
                # ä½¿ç”¨å®Œæ•´çš„VLMç¼–ç æ–‡æ¡£
                embedding = self.encode_single_document(doc_idx)
                doc_embeddings[doc_idx] = embedding
            except Exception as e:
                print(f"Error encoding document {doc_idx}: {e}")
                # å¦‚æœç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡
                doc_embeddings[doc_idx] = torch.zeros(4096).to(self.device)
        
        # ä½¿ç”¨åŸå§‹è¯„ä¼°å™¨çš„æ ¼å¼
        predictions = defaultdict(list)
        
        for idx, item in enumerate(tqdm(test_samples, desc="Testing Dual-Encoder")):
            query = item["query"]
            search_results = item.get("search_results", [])
            
            if not search_results:
                continue
            
            # ç¼–ç æŸ¥è¯¢ - ä½¿ç”¨å®Œæ•´VLMåŠŸèƒ½
            query_embedding = self.encode_query(query)
            
            # è·å–å€™é€‰æ–‡æ¡£å¹¶è®¡ç®—ç›¸ä¼¼åº¦
            candidates = search_results[:100]
            
            for note_idx in candidates:
                if note_idx in doc_embeddings:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = torch.cosine_similarity(
                        query_embedding.unsqueeze(0), 
                        doc_embeddings[note_idx].unsqueeze(0)
                    ).item()
                    
                    # ä½¿ç”¨idxä½œä¸ºqidï¼Œå› ä¸ºqrelsä¸­çš„qidæ˜¯ä»0å¼€å§‹çš„è¿ç»­æ•°å­—
                    qid = idx
                    predictions[qid].append((note_idx, similarity))
        
        # æŒ‰åŸå§‹è¯„ä¼°å™¨æ ¼å¼æ•´ç†ç»“æœ
        sorted_results = {}
        for qid, preds in predictions.items():
            sorted_results[qid] = [pid for pid, _ in sorted(preds, key=lambda x: x[1], reverse=True)]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç»“æœ
        print(f"Dual-encoder sorted_results keys: {list(sorted_results.keys())[:5]}")
        print(f"Number of queries with results: {len(sorted_results)}")
        print(f"Qrels keys sample: {list(self.qrels.keys())[:5]}")
        print(f"Number of qrels: {len(self.qrels)}")
        
        # ä½¿ç”¨åŸå§‹çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
        metrics = calculate_metrics(sorted_results, self.qrels, [1, 3, 5, 10])
        
        print("\nDual-Encoder Performance:")
        print("=" * 50)
        for k, v in metrics.items():
            print(f"{k}: {v:.4f}")
        print("=" * 50)
        
        return metrics, sorted_results
    
    def test_dual_encoder_quick(self, sample_num=2, doc_limit=5):
        """å¿«é€Ÿæµ‹è¯•åŒå‘ç¼–ç å™¨æ€§èƒ½ - åªå¤„ç†å°‘é‡æ•°æ®"""
        print(f"\n=== Quick Dual-Encoder Test (Sample: {sample_num}, Docs per query: {doc_limit}) ===")
        
        # ä½¿ç”¨åŸå§‹è¯„ä¼°å™¨çš„æ ¼å¼
        predictions = defaultdict(list)
        
        # é€‰æ‹©æµ‹è¯•æ ·æœ¬
        test_samples = self.test_data.select(range(min(sample_num, len(self.test_data))))
        
        for idx, item in enumerate(tqdm(test_samples, desc="Quick Dual-Encoder Test")):
            query = item["query"]
            search_results = item.get("search_results", [])
            
            if not search_results:
                continue
            
            print(f"\nQuery {idx}: {query[:]}...")
            
            # åªå¤„ç†å‰å‡ ä¸ªå€™é€‰æ–‡æ¡£
            candidates = search_results[:doc_limit]
            print(f"Processing {len(candidates)} documents...")
            
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.encode_query(query)
            
            # å¤„ç†æ¯ä¸ªå€™é€‰æ–‡æ¡£
            doc_details = []  # å­˜å‚¨æ–‡æ¡£è¯¦æƒ…ç”¨äºæ˜¾ç¤º
            for doc_idx, note_idx in enumerate(candidates):
                print(f"  Processing doc {doc_idx+1}/{len(candidates)}: {note_idx}")
                
                # è·å–æ–‡æ¡£å†…å®¹ç”¨äºæ˜¾ç¤º
                note = self.corpus[note_idx]
                doc_title = note['note_title']
                doc_content = note['note_content']
                doc_image_paths = note.get('image_path', [])
                
                # ç¼–ç æ–‡æ¡£
                doc_embedding = self.encode_single_document(note_idx)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = torch.cosine_similarity(
                    query_embedding.unsqueeze(0), 
                    doc_embedding.unsqueeze(0)
                ).item()
                
                print(f"    Similarity: {similarity:.4f}")
                print(f"    Title: {doc_title[:]}...")
                print(f"    Content: {doc_content[:]}...")
                if doc_image_paths:
                    print(f"    Images: {doc_image_paths}")
                else:
                    print(f"    Images: æ— å›¾ç‰‡")
                
                doc_details.append({
                    'note_idx': note_idx,
                    'title': doc_title,
                    'content': doc_content,
                    'image_paths': doc_image_paths,
                    'similarity': similarity
                })
                
                qid = idx
                predictions[qid].append((note_idx, similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶æ˜¾ç¤º
            doc_details.sort(key=lambda x: x['similarity'], reverse=True)
            # æ˜¾ç¤ºçœŸå®ç›¸å…³æ–‡æ¡£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            qid = idx
            if qid in self.qrels:
                relevant_docs = self.qrels[qid]
                print(f"\nâœ… Query {idx} çš„çœŸå®ç›¸å…³æ–‡æ¡£: {relevant_docs}")
                
                # æ˜¾ç¤ºçœŸå®ç›¸å…³æ–‡æ¡£çš„å†…å®¹
                print("ğŸ“– çœŸå®ç›¸å…³æ–‡æ¡£å†…å®¹:")
                for rel_doc_idx in relevant_docs[:]:
                    if rel_doc_idx < len(self.corpus):
                        rel_note = self.corpus[rel_doc_idx]
                        rel_image_paths = rel_note.get('image_path', [])
                        print(f"   ğŸ“„ æ–‡æ¡£ID {rel_doc_idx}:")
                        print(f"      æ ‡é¢˜: {rel_note['note_title']}")
                        print(f"      å†…å®¹: {rel_note['note_content'][:]}...")
                        if rel_image_paths:
                            print(f"      å›¾ç‰‡: {rel_image_paths}")
                        else:
                            print(f"      å›¾ç‰‡: æ— å›¾ç‰‡")
                        print("      " + "="*80)
            else:
                print(f"\nâŒ Query {idx} åœ¨qrelsä¸­æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
            
            print(f"\nğŸ“‹ Query {idx} VLMæ’åºç»“æœ:")
            for rank, doc in enumerate(doc_details, 1):
                # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸå®ç›¸å…³æ–‡æ¡£
                is_relevant = "âœ…" if qid in self.qrels and doc['note_idx'] in self.qrels[qid] else "âŒ"
                print(f"  {rank}. {is_relevant} [ç›¸ä¼¼åº¦: {doc['similarity']:.4f}]")
                print(f"     æ ‡é¢˜: {doc['title']}")
                print(f"     å†…å®¹: {doc['content'][:]}...")
                if doc['image_paths']:
                    print(f"     å›¾ç‰‡: {doc['image_paths']}")
                else:
                    print(f"     å›¾ç‰‡: æ— å›¾ç‰‡")
                print(f"     æ–‡æ¡£ID: {doc['note_idx']}")
                print("     " + "-"*80)
        
        # æŒ‰åŸå§‹è¯„ä¼°å™¨æ ¼å¼æ•´ç†ç»“æœ
        sorted_results = {}
        for qid, preds in predictions.items():
            sorted_results[qid] = [pid for pid, _ in sorted(preds, key=lambda x: x[1], reverse=True)]
            print(f"Query {qid} ranking: {sorted_results[qid]}")
        
        # ä½¿ç”¨åŸå§‹çš„æŒ‡æ ‡è®¡ç®—å‡½æ•°
        metrics = calculate_metrics(sorted_results, self.qrels, [1, 3, 5])
        
        print("\nQuick Dual-Encoder Performance:")
        print("=" * 50)
        for k, v in metrics.items():
            print(f"{k}: {v:.4f}")
        print("=" * 50)
        
        return metrics, sorted_results

    def compute_cross_encoder_score(self, query, note_idx):
        """è®¡ç®—äº¤å‰ç¼–ç å™¨åˆ†æ•° - ä½¿ç”¨æ¨¡å‹éšè—çŠ¶æ€è¾“å‡ºåˆ†æ•°è€Œä¸æ˜¯ç”Ÿæˆæ–‡æœ¬"""
        try:
            # è·å–æ–‡æ¡£å†…å®¹ - å‚è€ƒåŸå§‹æ•°æ®å¤„ç†å™¨
            note = self.corpus[note_idx]
            doc_title = note['note_title']
            doc_content = note['note_content']
            doc_image = self.load_note_image(note.get('image_path', []))
            
            # æ„å»ºå¯¹è¯ - å‚è€ƒåŸå§‹VLMæ•°æ®å¤„ç†å™¨
            conversation = [{
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": f"ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{query}\nç¬”è®°å†…å®¹æ˜¯ï¼š{doc_title} {doc_content}\nè¯·ä½ åˆ¤æ–­ç¬”è®°æ˜¯å¦ç›¸å…³ï¼Œå¦‚æœå›¾ç‰‡ä¸æ˜¯ç©ºç™½ï¼Œåˆ™ä¹Ÿè€ƒè™‘å›¾ç‰‡å†…å®¹ã€‚"}
                ]
            }]
            
            # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šå…ˆç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œç„¶åç»Ÿä¸€å¤„ç†
            text_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=False)
            
            # ä½¿ç”¨processorç»Ÿä¸€å¤„ç†æ–‡æœ¬å’Œå›¾åƒ - å…³é”®ä¿®å¤ï¼Œç¦ç”¨æˆªæ–­
            inputs = self.processor(
                text=[text_prompt],
                images=[doc_image],
                padding=True,
                return_tensors="pt"
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ä½¿ç”¨æ¨¡å‹ç›´æ¥è¾“å‡ºç›¸å…³æ€§åˆ†æ•° - å‚è€ƒåŸå§‹VLMè¯„ä¼°å™¨
            with torch.no_grad():
                outputs = self.model(**inputs, output_hidden_states=True)
                # ä½¿ç”¨æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å¹³å‡å€¼ä½œä¸ºç›¸å…³æ€§åˆ†æ•°
                hidden_states = outputs.hidden_states[-1]
                score = hidden_states.mean().item()
                
                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
                normalized_score = torch.sigmoid(torch.tensor(score)).item()
            
            # è°ƒè¯•ï¼šæ‰“å°åˆ†æ•°
            print(f"Raw score for note {note_idx}: {score:.4f}, Normalized: {normalized_score:.4f}")
            
            return normalized_score
            
        except Exception as e:
            print(f"Error computing score for note {note_idx}: {e}")
            return 0.0
        
    
    def encode_query(self, query):
        """ç¼–ç æŸ¥è¯¢ - ä½¿ç”¨åŸå§‹VLMæ–¹æ³•"""
        # ä¸ºæŸ¥è¯¢åˆ›å»ºé»˜è®¤å›¾åƒä»¥ä¿æŒè¾“å…¥æ ¼å¼ä¸€è‡´
        default_image = self.create_default_image()
        
        conversation = [{
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"æŸ¥è¯¢ï¼š{query}"}
            ]
        }]
        
        # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šå…ˆç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œç„¶åç»Ÿä¸€å¤„ç†
        text_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=False)
        
        # ä½¿ç”¨processorç»Ÿä¸€å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œç¦ç”¨æˆªæ–­
        inputs = self.processor(
            text=[text_prompt],
            images=[default_image],
            padding=True,
            return_tensors="pt"
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            # ä½¿ç”¨æœ€åä¸€å±‚çš„å¹³å‡æ± åŒ–
            hidden_states = outputs.hidden_states[-1]
            query_embedding = hidden_states.mean(dim=1)  # [1, hidden_size]
        
        return query_embedding.squeeze(0)
    
    def encode_query_simple(self, query):
        """ç®€åŒ–æŸ¥è¯¢ç¼–ç """
        try:
            inputs = self.processor.tokenizer(
                f"æŸ¥è¯¢ï¼š{query}",
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.model(**inputs, output_hidden_states=True)
                hidden_states = outputs.hidden_states[-1]
                query_embedding = hidden_states.mean(dim=1)
            
            return query_embedding.squeeze(0)
        except Exception as e:
            print(f"Error encoding query: {e}")
            return torch.zeros(4096).to(self.device)
    
    def encode_document_simple(self, note_idx):
        """ç®€åŒ–æ–‡æ¡£ç¼–ç """
        try:
            note = self.corpus[note_idx]
            doc_title = note['note_title']
            doc_content = note['note_content']
            
            inputs = self.processor.tokenizer(
                f"æ–‡æ¡£ï¼š{doc_title} {doc_content}",
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.model(**inputs, output_hidden_states=True)
                hidden_states = outputs.hidden_states[-1]
                doc_embedding = hidden_states.mean(dim=1)
            
            return doc_embedding.squeeze(0)
        except Exception as e:
            print(f"Error encoding document {note_idx}: {e}")
            return torch.zeros(4096).to(self.device)
    
    def encode_single_document(self, note_idx):
        """ç¼–ç å•ä¸ªæ–‡æ¡£ - ä½¿ç”¨åŸå§‹VLMæ–¹æ³•"""
        note = self.corpus[note_idx]
        doc_title = note['note_title']
        doc_content = note['note_content']
        doc_image = self.load_note_image(note.get('image_path', []))
        
        conversation = [{
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"æ–‡æ¡£æ ‡é¢˜ï¼š{doc_title}\næ–‡æ¡£å†…å®¹ï¼š{doc_content}"}
            ]
        }]
        
        # ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šå…ˆç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œç„¶åç»Ÿä¸€å¤„ç†
        text_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=False)
        
        # ä½¿ç”¨processorç»Ÿä¸€å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼Œç¦ç”¨æˆªæ–­
        inputs = self.processor(
            text=[text_prompt],
            images=[doc_image],
            padding=True,
            return_tensors="pt"
        )
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]
            doc_embedding = hidden_states.mean(dim=1)  # [1, hidden_size]
        
        return doc_embedding.squeeze(0)
    
    def encode_documents_batch(self, doc_indices):
        """æ‰¹é‡ç¼–ç æ–‡æ¡£"""
        conversations = []
        images = []
        
        for note_idx in doc_indices:
            note = self.corpus[note_idx]
            doc_title = note['note_title']
            doc_content = note['note_content']
            doc_image = self.load_note_image(note.get('image_path', []))
            
            conversation = [{
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": f"æ–‡æ¡£æ ‡é¢˜ï¼š{doc_title}\næ–‡æ¡£å†…å®¹ï¼š{doc_content}"}
                ]
            }]
            
            conversations.append(conversation)
            images.append(doc_image)
        
        # æ‰¹é‡å¤„ç† - æ·»åŠ padding
        inputs = self.processor.apply_chat_template(
            conversations, 
            tokenize=True, 
            add_generation_prompt=False,
            return_tensors="pt",
            padding=True
        )
        
        # åˆ†æ­¥å¤„ç†å›¾åƒä»¥é¿å…ç´¢å¼•é”™è¯¯
        try:
            image_inputs = self.processor.image_processor(
                images, return_tensors="pt"
            )
            inputs['pixel_values'] = image_inputs['pixel_values']
        except Exception as e:
            print(f"Error processing images: {e}")
            print(f"Number of images: {len(images)}")
            # å¦‚æœå›¾åƒå¤„ç†å¤±è´¥ï¼Œå¤„ç†æ¯ä¸ªå›¾åƒå•ç‹¬
            pixel_values_list = []
            for i, img in enumerate(images):
                try:
                    img_inputs = self.processor.image_processor([img], return_tensors="pt")
                    pixel_values = img_inputs['pixel_values']
                    print(f"Image {i} pixel_values shape: {pixel_values.shape}")
                    
                    # æ£€æŸ¥å½¢çŠ¶å¹¶é‡å¡‘ä¸ºæ­£ç¡®çš„4ç»´å¼ é‡
                    if len(pixel_values.shape) == 2:
                        # å¦‚æœæ˜¯2ç»´ï¼Œéœ€è¦é‡å¡‘ä¸ºåˆé€‚çš„4ç»´å¼ é‡
                        # å‡è®¾è¿™æ˜¯å·²ç»flattençš„å›¾åƒæ•°æ®ï¼Œå°è¯•é‡å¡‘
                        # è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…çš„å›¾åƒå¤„ç†å™¨è¾“å‡ºè°ƒæ•´
                        print(f"Warning: 2D pixel_values detected, shape: {pixel_values.shape}")
                        # æš‚æ—¶è·³è¿‡è¿™ä¸ªå›¾åƒæˆ–ä½¿ç”¨é»˜è®¤å¤„ç†
                        continue
                    else:
                        pixel_values_list.append(pixel_values)
                        
                except Exception as img_e:
                    print(f"Error processing individual image {i}: {img_e}")
                    continue
                    
            if pixel_values_list:
                try:
                    inputs['pixel_values'] = torch.cat(pixel_values_list, dim=0)
                except Exception as cat_e:
                    print(f"Error concatenating pixel_values: {cat_e}")
                    # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾åƒçš„å€¼
                    inputs['pixel_values'] = pixel_values_list[0]
            else:
                print("No valid pixel_values found, returning zero embeddings")
                return torch.zeros(len(doc_indices), 4096)
        
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]
            doc_embeddings = hidden_states.mean(dim=1)  # [batch_size, hidden_size]
        
        return doc_embeddings
    
    def calculate_query_metrics(self, ranked_docs, relevant_docs):
        """è®¡ç®—å•ä¸ªæŸ¥è¯¢çš„æŒ‡æ ‡"""
        metrics = {}
        
        # è®¡ç®—å„ç§kå€¼çš„æŒ‡æ ‡
        for k in [1, 3, 5, 10]:
            hits_at_k = [doc in relevant_docs for doc in ranked_docs[:k]]
            
            # MRR@k
            if any(hits_at_k):
                first_hit_rank = hits_at_k.index(True) + 1
                metrics[f'MRR@{k}'] = 1.0 / first_hit_rank
            else:
                metrics[f'MRR@{k}'] = 0.0
            
            # Recall@k
            metrics[f'Recall@{k}'] = sum(hits_at_k) / len(relevant_docs)
            
            # Precision@k
            metrics[f'Precision@{k}'] = sum(hits_at_k) / k
        
        return metrics
    
    def save_results(self, dual_encoder_results, output_dir):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æŒ‡æ ‡
        results_summary = {
            'dual_encoder': dual_encoder_results[0]
        }
        
        with open(os.path.join(output_dir, 'performance_metrics.json'), 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        print(f"\nResults saved to: {output_dir}")
        print("Performance Summary:")
        print("Dual-Encoder:", dual_encoder_results[0])

def main():
    parser = argparse.ArgumentParser(description='Test VLM performance on Qilin dataset')
    parser.add_argument('--model_path', type=str, required=True, 
                       help='Path to VLM model')
    parser.add_argument('--sample_num', type=int, default=100,
                       help='Number of test samples')
    parser.add_argument('--output_dir', type=str, default='vlm_test_results',
                       help='Output directory for results')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = VLMPerformanceTester(args.model_path, args.device)
    
    # æµ‹è¯•äº¤å‰ç¼–ç å™¨æ€§èƒ½
    # cross_encoder_results = tester.test_cross_encoder_performance(args.sample_num)
     # æµ‹è¯•åŒå‘ç¼–ç å™¨æ€§èƒ½
    # dual_encoder_results = tester.test_dual_encoder_performance(args.sample_num)
    # æµ‹è¯•åŒå‘ç¼–ç å™¨æ€§èƒ½ - ä½¿ç”¨å¿«é€Ÿæµ‹è¯•
    dual_encoder_results = tester.test_dual_encoder_quick(sample_num=args.sample_num, doc_limit=10)
    
    # ä¿å­˜ç»“æœ
    tester.save_results(dual_encoder_results, args.output_dir)

if __name__ == "__main__":
    main()
