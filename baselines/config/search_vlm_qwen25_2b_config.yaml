project_name: "search_vlm_qwen25_2b"
project_dir: "output_search_vlm_qwen25_2b"
trainer: vlm_trainer

# Model Configuration
model:
  model_name_or_path: "../model/Qwen2.5-VL-2B-Instruct/"  # 更新为Qwen2.5-VL
  tokenizer_name_or_path: "../model/Qwen2.5-VL-2B-Instruct/"  # 更新为Qwen2.5-VL
  load_from_new: false  # Whether to load from the latest checkpoint
  lora_checkpoint_dir: "vlm_qwen25_2b_checkpoints/"  # LoRA checkpoint save path
  gradient_checkpointing: true  # Enable gradient checkpointing to save GPU memory
  load_in_4bit: true
  user_lora: true

# Dataset Configuration
datasets:
  dataset_name_or_path: "../datasets/qilin/"  # Dataset path
  train_data_processor: "VLMTrainingDataProcessor"  # Training data processor
  tokenizer_name_or_path: "../model/Qwen2.5-VL-2B-Instruct/"
  batch_size: 64  # Batch size
  eval_batch_size: 1
  max_length: 512  # Maximum sequence length
  negative_samples: 1  # Number of negative samples per query
  use_title: true  # Whether to use title
  use_content: true  # Whether to use content

# Training Configuration
training:
  num_epochs: 10000  # Number of training epochs
  eval_steps: 500  # Evaluation steps
  save_steps: 500  # Save steps
  eval_epochs: 1  # Evaluate every N epochs
  save_epochs: 1  # Save model every N epochs

# Optimizer Configuration
optimizer: 
  name: AdamW  # Use AdamW optimizer
  kwargs:
    lr: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

# Learning Rate Scheduler Configuration
scheduler:
  name: LinearLR
  kwargs:
    total_iters: 100

# Evaluation Configuration
evaluation:
  target_metric: "MRR@10"  # Target evaluation metric
  output_dir: "eval_results"  # Evaluation results output directory
  qrels_data_path: "../datasets/search.test.qrels.csv"  # Evaluation data path
  rerank_depth: 1000  # Reranking depth
  # results_key: "bm25_results"  # Retrieved results to use
  results_key: "search_results"  # Use XHS exposure results
  sample_num: 1000  # Number of evaluation samples, evaluate first {sample_num} samples

# Logger Configuration
logger:
  log_with: "tensorboard"  # Use tensorboard for training log recording
